# Deep Learning Manager -- How to Use 

---

The deep learning manager can be installed in two different ways. 
If one plans to run the deep learning manager through Docker, 
then cloning the repository is sufficient. Otherwise, additional setup 
steps are required.

## Installation (running through Docker)

Clone the repository

```shell 
git clone https://github.com/mining-design-decisions/maestro-dl-manager.git
```

## Installation (running directly)

The Deep Learning Manager requires `Python` (`>= 3.10`) and `Rust` (`>= 1.60`) to be installed.

First, the code for the deep learning manager should be cloned by running the following command:

```shell 
git clone https://github.com/mining-design-decisions/maestro-dl-manager.git
```

Next, `cd` into the cloned directory:

```shell 
cd maestro-dl-manager 
```

Next, create a virtual environment in which the Python dependencies can be installed:

```shell 
python -m venv venv 
```

Next, start the virtual environment:

```shell 
source venv/bin/activate 
```

Now, install the build dependencies:

```shell 
python -m pip install nltk setuptools rust_setuptools 
```

Next, build the acceleration module:

```shell 
python setup.py build_ext --inplace 
```

And finally, install the remaining requirements:

```shell 
python setup.py sdist 
python -m pip install -r dl_manager.egg-info/requires.txt
```

---

## Preparations Before Running 

If the pipeline will be deployed as a web server (normal operation mode),
then SLL key and certificate files must be present to enable secure HTTPS 
connections.

For testing purposes or local deployments, self-signed certificates can 
be generated by running the following command:

```shell 
openssl req -new -x509 -nodes -sha256 -out server.crt -keyout server.key
```

---

## Running Using Docker 

When running using docker, one most first determine whether the pipeline will be 
run using CUDA. If the pipeline will be run using CUDA, the appropriate Docker 
drives must first be installed using 

```shell  
./prepare_docker_for_gpu.sh 
```

Next, to run with CUDA enabled, run the command 
```shell 
docker compose -f docker-compose-gpu.yml up --build 
```

If running without GPU acceleration, run 
```shell 
docker compose -f docker-compose-no-gpu.yml up --build 
 ```

We also have another docker compose (`docker-compose-gpu-alt.yml``) for running with other GPUs 
using the [TensorFlow-DirectML-Plugin](https://github.com/microsoft/tensorflow-directml-plugin).
However, this compose file has not been tested and is unlikely to work.

## Running Directly 

To run the pipeline (in web server mode) directly, run 

```shell
python -m dl_manager --keyfile path/to/key/file --certfile path/to/certificate/file 
```

It is also possible to configure the port on which the server will listen to connections.
This can be done using the `--port` flag. The default is `9011`.

## Running in Scripting Mode

The deep learning manager can be run in a special scripting mode. In this mode,
it will run a pre-specified set of commands. This is mainly intended for running 
predefined sets of task in environments where running as a web server is 
not possible (e.g. certain high performance computing clusters).

To run the manager in scripting mode, run 

```shell 
python -m dl_manager --script path/to/script/file
```

Here, the scripting file contains the commands to be run.
The following is an example file:

```json 
{
    "auth": {
        "token": "<token>",
        "token-endpoint": "https://database-url:port/refresh-token"
    },
    "script": [
        {
            "cmd": "generate-embedding",
            "args": {
                "embedding-id": "6491b86127a779d274330609",
                "database-url": "https://database-url:port"
            }
        },
        {
            "cmd": "train",
            "args": {
                "model-id": "6492208e27a779d274338838",
                "database-url": "https://database-url:port"
            }
        }
    ]
}
```

Here, the token must be obtained by authentication with the database API using the 
`/token` endpoint.

By default, the deep learning manager will use a checkpointing mechanism.
If it fails while running a command and is restarted with the same file, it will
not re-execute all commands that were executed successfully prior to the crash.
To overwrite this behaviour, pass the `--invalidate-checkpoints` flag.

Note that when the manager crashes while executing a command, any modified 
database state will **not** be reverted.

## [UG] Deployment On Hábrók

For users from the University of Groningen, we have special instructions for deploying
the deep learning manager on Hábrók. First of all, the deep learning manager 
must be prepared for running directly.

Note that, prior to installation, the Rust and Python modules must first be loaded.
The following two commands have been verified to work:

```shell 
module load TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0
module load Rust/1.60.0-GCCcore-11.3.0
```
The deep learning manager must be run in scripting mode. 
Additionally, the following environment variable must be set:

```shell 
export DL_MANAGER_RUNNING_ON_PEREGRINE=true 
```

The following is an example job script which can be used to run the deep learning manager 
on Hábrók:

```shell
#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --nodes 1
#SBATCH --ntasks 1
#SBATCH --cpus-per-task 1
#SBATCH --mem=24GB
#SBATCH --job-name=deeplearning-test-run
#SBATCH --partition=regular

module load TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0
module load Rust/1.60.0-GCCcore-11.3.0

source ./venv/bin/activate

export DL_MANAGER_RUNNING_ON_PEREGRINE=TRUE
export DL_MANAGER_NUM_THREADS=1

python3.10 -m dl_manager --script my_script.json 
```

## Environment Variables

- `DL_MANAGER_RUNNING_ON_PEREGRINE`: A custom tag for use by users from the University of Groningen. 
    Must be set to `true` 
- `DL_MANAGER_ALLOW_UNSAFE_SSL`: Set to `true` when the deep learning manager is allowed to 
    connect to the database API when the database API is deployed using self-signed SSL certificates.
- `DL_MANAGER_NUM_THREADS`: integer specifying how many threads the system is allowed to use.
    Currently only affects the numbers of threads using while preprocessing data.

---

## How To Use

Once the manager is running, a list of available endpoints is available from the `/docs` endpoint.
The `/endpoints`, `/arglists/*`, `/constraints`, and `/*/dynamic-enum` endpoints are mostly for 
internal usage, and provide a runtime description of the parameters accepted by the 
deep learning manager. 

In the remainder of this section, we give a description of the various public 
endpoints and their parameters. 
We advise reading the [detailed explanation of parameter types](./param-types.md)
before continuing.

The remaining endpoints are the endpoints intended for public usage.
Currently, there are the following endpoints:

### `generate-embedding` (and `generate-embedding-internal`)
  
Endpoint for generating (training) embeddings, to be used when training models.
By `embedding`, we mean either a semantic word embedding, a dictionary of words 
to be used by bag of words models, or a dictionary of words combined with 
IDF weights for TF-IDF models.

The public facing part of this endpoint if the `generate-embedding` endpoint.
Its list of required arguments can be found [here](./dl_manager_endpoint__generate-embedding.md).
One of its arguments is the ID of an _embedding config_ stored in the database.
The contents of this embedding config are passed as additional parameters to the 
`generate-embedding-internal` endpoint. The arguments for that endpoint can be found 
[here](./dl_manager_endpoint__generate-embedding-internal.md).

### `train` (and `run`)

Endpoint for training and evaluating classifiers. Internally, the `/train` endpoint
(arguments described [here](./dl_manager_endpoint__train.md))
retrieves a model config from the database, and passes all parameters in the config 
as arguments to the `run` endpoint. This endpoint is very big and has 
many arguments, which are described [here](./dl_manager_endpoint__run.md).

On a high level, one must specify a feature generator (i.e. an algorithm to generate features).
Often, this requires the specification of a previously trained embedding. Next, a model 
must be specified which is trained and evaluated using the generated features. 
After training, the trained models are stored in the database. This endpoint returns the 
IDs of all stored model, the performance files in the database containing the evaluation
information, and any other auxiliary files stored in the database during training.

### `predict`

The predict command is used to run a pre-trained model on unseen data. 
The description of its argument can be found 
[here](./dl_manager_endpoint__predict.md)

### `metrics`

The metrics endpoint is used to evaluate a trained model, by computing performance 
metrics from the evaluation files stored by the `train` endpoint.
The arguments of the `metrics` endpoint can be found 
[here](./dl_manager_endpoint__metrics.md)

### `confusion-matrix`

The confusion matrix endpoint is similar to the metrics endpoint, but 
is specifically used for generating confusion matrices. 
Its arguments can be found [here](./dl_manager_endpoint__confusion-matrix.md)






